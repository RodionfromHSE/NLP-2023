{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slide 1: Introduction to k-order Approximations\n",
    "In information theory, **k-order approximations** are methods for predicting symbols. Different orders (0 to k) provide varying levels of context for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Slide 2: 0-order Approximation (Uniform Probabilities)\n",
    "In **0-order approximation**, all symbols are assumed to have **uniform probabilities**. For example, in the sequence \"aabcaaccaabaa,\" each symbol has an equal chance of occurring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Slide 3: 1-order Approximation (Probabilities According to Symbols Distribution)\n",
    "In **1-order approximation**, probabilities are based on the **distribution of individual symbols**. For instance, in \"aabcaaccaabaa,\" the probability of 'a' is higher than 'b' due to their frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Slide 4: 2-order Approximation (Predict Symbol Based on Previous One)\n",
    "In **2-order approximation**, symbols are predicted based on the **previous symbol**. For example, considering \"aabcaaccaabaa,\" if the previous symbol is 'a,' the next symbol is likely to be 'a' as well. \n",
    "Here $P(a|a) = \\frac{\\#(aa)}{\\#(a)} = \\frac{4}{8} = 0.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Slide 5: k-order Approximation (Predict Symbol Based on Previous k-1 Symbols)\n",
    "**k-order approximation** predicts symbols based on the **previous k-1 symbols**. It offers a more nuanced understanding of patterns. For \"aabcaaccaabaa,\" a 2-order approximation would group symbols like \"aa, b, c, aa, cc, aa, b, a, a.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Slide 6: Entropy - Quantifying Uncertainty\n",
    "**Entropy** measures uncertainty in data. In a uniform distribution, entropy is maximum. Shannon's formula, \\(H(X) = - \\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i)\\), quantifies this uncertainty based on probabilities of outcomes.\n 
     Uncertainty Measurement: Entropy quantifies the level of uncertainty associated with a random variable or source. When there is high uncertainty, entropy is high, indicating that the source is unpredictable. Conversely, when the source is highly predictable, entropy is low.
"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Slide 7: Connection Between \"What is Information?\" and Shannon's Reformulation\n",
    "Traditionally, information was vague. Shannon redefined it as **reduction of uncertainty**. Entropy, H(X), quantifies this. More uncertain data has higher entropy, reflecting our surprise or lack of information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Slide 8: Derivation of Entropy Formula in Uniform Case\n",
    "In a **uniform distribution**, where all outcomes are equally likely, entropy is maximized. Shannon's entropy formula sums over all possible outcomes, weighting each by its probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Slide 9: Cross Entropy - Comparing Distributions\n",
    "**Cross entropy** compares two probability distributions: the predicted one (q) and the actual one (p). It evaluates how well q predicts the actual distribution p.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slide 10: Why Cross Entropy?\n",
    "In real applications, such as **machine learning**, we often predict probabilities (q) and need to compare them with true probabilities (p). Cross entropy measures the efficiency of predicting p using q.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slide 11: Cross Entropy Definition and Example\n",
    "**Cross entropy between p and q:** \\(H(p, q) = - \\sum_{i=1}^{n} p(x_i) \\log_2 q(x_i)\\). For example, if p = [0.2, 0.8] and q = [0.6, 0.4], cross entropy measures the difference between predicted and actual probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slide 12: Recap and Applications\n",
    "- **0-order:** Uniform probabilities.\n",
    "- **1-order:** Probabilities based on symbol distribution.\n",
    "- **2-order:** Predict symbols based on previous ones.\n",
    "- **k-order:** Predict symbols based on previous k-1 symbols.\n",
    "- **Entropy:** Quantifies uncertainty in data.\n",
    "- **Cross Entropy:** Measures prediction efficiency.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
