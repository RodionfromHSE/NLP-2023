{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"arxivData.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: Dual Recurrent Attention Units for Visual Question Answering\\nSummary: We propose an architect'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentences(n):\n",
    "    model = markovify.Text(text, state_size=n)\n",
    "    sentences = [model.make_sentence(tries=1000) for _ in range(5)]\n",
    "    return sentences\n",
    "\n",
    "def print_sentences(sentences):\n",
    "    for i, s in enumerate(sentences):\n",
    "        print(\"Sentence\", i+1)\n",
    "        print(s)\n",
    "        print('-'*10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "In addition, it is to zero mean algorithms, to be.\n",
      "----------\n",
      "Sentence 2\n",
      "Thereafter, UML class of learning.\n",
      "----------\n",
      "Sentence 3\n",
      "Simulation results was tested in a semi-supervised setting: DeCA.\n",
      "----------\n",
      "Sentence 4\n",
      "We further improvement this paper as Neuro-Fuzzy Techniques for noise by the era of the simplification is to accelerate ML estimator performs particularly important during manipulation.\n",
      "----------\n",
      "Sentence 5\n",
      "The non-finite-state copying aspect of training data representation languages.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sen1 = gen_sentences(1)\n",
    "print_sentences(sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "Among other things, with these two popular classifiers are usually significantly lower.\n",
      "----------\n",
      "Sentence 2\n",
      "A smaller version of the proposed algorithm to deduce the underlying algorithm is experimentally verified.\n",
      "----------\n",
      "Sentence 3\n",
      "Title: Anytime Induction of Word Embeddings By Word Sense Disambiguation Summary: This document describes the new model layer, OpenMax, which estimates the appropriate lexical usage we combine the inherent drawback that the discrete-continuous models outperform each single task.\n",
      "----------\n",
      "Sentence 4\n",
      "Furthermore, we show that our technique for clustering data which is related to a frequently used indicator of SAT 2011, Glucose improved by optimizing the target dataset is feeded with secondary voltages to the inherent uncertainty and vagueness will yield a robust modification of all astronomical archives available worldwide, as well as automatically illustrating recipes with keyframes, and searching are implemented.\n",
      "----------\n",
      "Sentence 5\n",
      "The result is a branch and bound algorithm that attains $68.1\\%$ accuracy on MOT16 challenge, compared to the communication topology of a grammar.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sen2 = gen_sentences(2)\n",
    "print_sentences(sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "Among other results, we show that the difference between the probability distributions are not known beforehand, or so easily delineated, however.\n",
      "----------\n",
      "Sentence 2\n",
      "Genotypes are strings of tokens of a given set of music clips.\n",
      "----------\n",
      "Sentence 3\n",
      "When working with massive data, we show how to use and adapt PonyGE2 have been developed.\n",
      "----------\n",
      "Sentence 4\n",
      "Finally, we evaluate our conversion process by using the learned hidden sentence representations.\n",
      "----------\n",
      "Sentence 5\n",
      "Title: Multi-Instance Visual-Semantic Embedding Summary: Visual-semantic embedding models have been found, including caching, higher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and clustering.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sen3 = gen_sentences(3)\n",
    "print_sentences(sen3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "Third, to reduce the synchronization cost, we terminate the process of finding an good ESN for a specific dataset quite hard.\n",
      "----------\n",
      "Sentence 2\n",
      "The proposed algorithms are amenable to parallelization, scale linearly in the size of output space in the worst-case.\n",
      "----------\n",
      "Sentence 3\n",
      "By open-sourcing our framework, we hope to stimulate progress in the field of semantic segmentation and object detection.\n",
      "----------\n",
      "Sentence 4\n",
      "Title: Kernels for sequentially ordered data Summary: We present a nonparametric Bayesian method for exploratory data analysis and feature construction in continuous time series.\n",
      "----------\n",
      "Sentence 5\n",
      "An extensive empirical analysis shows that the proposed method is able to provide comparative performance for audio emotion recognition.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "sen5 = gen_sentences(5)\n",
    "print_sentences(sen5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 147.5555877685547\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the input sentence\n",
    "sentence = \"This is an example sentence.\"\n",
    "\n",
    "# Tokenize the input sentence\n",
    "input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Pass the tokenized input to the model and compute the loss\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "\n",
    "# Calculate the perplexity\n",
    "perplexity = torch.exp(loss).item()\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity for 1-gram sentences: 599.1120056152344\n",
      "Average perplexity for 2-gram sentences: 279.3132843017578\n",
      "Average perplexity for 3-gram sentences: 226.4117202758789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def average_perplexity(sentences):\n",
    "    total = 0\n",
    "    for s in sentences:\n",
    "        input_ids = tokenizer.encode(s, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        total += perplexity\n",
    "    return total/len(sentences)\n",
    "\n",
    "print(\"Average perplexity for 1-gram sentences:\", average_perplexity(sen1))\n",
    "print(\"Average perplexity for 2-gram sentences:\", average_perplexity(sen2))\n",
    "print(\"Average perplexity for 3-gram sentences:\", average_perplexity(sen3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity for 5-gram sentences: 138.5129737854004\n"
     ]
    }
   ],
   "source": [
    "print(\"Average perplexity for 5-gram sentences:\", average_perplexity(sen5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
